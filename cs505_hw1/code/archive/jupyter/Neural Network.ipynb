{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_file(data_file, is_test_file = False):\n",
    "    \"\"\"Load newsframing data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        First element is a list of strings(headlines)\n",
    "        If `data_file` has labels, the second element\n",
    "        will be a list of labels for each headline. \n",
    "        Otherwise, the second element will be None.\n",
    "    \"\"\"\n",
    "    print(\"Loading from {} ...\".format(data_file.name), end=\"\")\n",
    "    text_col = \"news_title\"\n",
    "    theme1_col = \"Q3 Theme1\"\n",
    "\n",
    "    with open(data_file) as f:\n",
    "        df = pd.read_csv(f, sep=\"\\t\")\n",
    "        X = df[text_col].tolist()\n",
    "\n",
    "        y = None\n",
    "        if not is_test_file:\n",
    "            if theme1_col in df.columns:\n",
    "                y = df[theme1_col].tolist()\n",
    "\n",
    "        print(\n",
    "            \"loaded {} lines {} labels ... done\".format(\n",
    "                len(X), \"with\" if y is not None else \"without\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return (X, y)\n",
    "\n",
    "def load_data_file_2(data_file, is_test_file = False):\n",
    "    \"\"\"Load newsframing data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        First element is a list of strings(headlines)\n",
    "        If `data_file` has labels, the second element\n",
    "        will be a list of labels for each headline. \n",
    "        Otherwise, the second element will be None.\n",
    "    \"\"\"\n",
    "    print(\"Loading from {} ...\".format(data_file.name), end=\"\")\n",
    "    text_col = \"news_title\"\n",
    "    theme1_col = \"Q3 Theme1\"\n",
    "\n",
    "    with open(data_file) as f:\n",
    "        df = pd.read_csv(f, sep=\"\\t\")\n",
    "        X = df[text_col].tolist()\n",
    "\n",
    "        y = None\n",
    "        if not is_test_file:\n",
    "            if theme1_col in df.columns:\n",
    "                y = df[theme1_col].tolist()\n",
    "\n",
    "        df = pd.DataFrame({'x': X, 'y': y})\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from train.tsv ...loaded 1040 lines with labels ... done\n",
      "Loading from dev.tsv ...loaded 130 lines with labels ... done\n",
      "Loading from test.tsv ...loaded 130 lines without labels ... done\n",
      "num_classes 9\n",
      "Train on 988 samples, validate on 52 samples\n",
      "Epoch 1/15\n",
      "988/988 [==============================] - 1s 1ms/step - loss: 1.9197 - accuracy: 0.3320 - val_loss: 1.6523 - val_accuracy: 0.3462\n",
      "Epoch 2/15\n",
      "988/988 [==============================] - 1s 678us/step - loss: 1.3478 - accuracy: 0.6093 - val_loss: 1.3569 - val_accuracy: 0.5769\n",
      "Epoch 3/15\n",
      "988/988 [==============================] - 1s 686us/step - loss: 0.8973 - accuracy: 0.7824 - val_loss: 1.1356 - val_accuracy: 0.6346\n",
      "Epoch 4/15\n",
      "988/988 [==============================] - 1s 857us/step - loss: 0.5834 - accuracy: 0.8775 - val_loss: 1.0003 - val_accuracy: 0.6731\n",
      "Epoch 5/15\n",
      "988/988 [==============================] - 1s 1ms/step - loss: 0.3790 - accuracy: 0.9281 - val_loss: 0.9620 - val_accuracy: 0.6923\n",
      "Epoch 6/15\n",
      "988/988 [==============================] - 1s 696us/step - loss: 0.2454 - accuracy: 0.9666 - val_loss: 0.9709 - val_accuracy: 0.6923\n",
      "Epoch 7/15\n",
      "988/988 [==============================] - 1s 729us/step - loss: 0.1666 - accuracy: 0.9828 - val_loss: 1.0174 - val_accuracy: 0.6923\n",
      "Epoch 8/15\n",
      "988/988 [==============================] - 1s 834us/step - loss: 0.1174 - accuracy: 0.9899 - val_loss: 1.0323 - val_accuracy: 0.7115\n",
      "Epoch 9/15\n",
      "988/988 [==============================] - 1s 699us/step - loss: 0.0863 - accuracy: 0.9980 - val_loss: 1.0749 - val_accuracy: 0.6731\n",
      "Epoch 10/15\n",
      "988/988 [==============================] - 1s 745us/step - loss: 0.0633 - accuracy: 0.9980 - val_loss: 1.0996 - val_accuracy: 0.6923\n",
      "Epoch 11/15\n",
      "988/988 [==============================] - 1s 742us/step - loss: 0.0463 - accuracy: 0.9990 - val_loss: 1.1416 - val_accuracy: 0.6731\n",
      "Epoch 12/15\n",
      "988/988 [==============================] - 1s 768us/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 1.1623 - val_accuracy: 0.6923\n",
      "Epoch 13/15\n",
      "988/988 [==============================] - 1s 751us/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 1.1992 - val_accuracy: 0.6731\n",
      "Epoch 14/15\n",
      "988/988 [==============================] - 1s 808us/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 1.2287 - val_accuracy: 0.6731\n",
      "Epoch 15/15\n",
      "988/988 [==============================] - 1s 744us/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 1.2604 - val_accuracy: 0.6923\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FILE = Path(\"raw_data/GunViolence/train.tsv\")\n",
    "BAL_TRAIN_FILE = Path(\"raw_data/GunViolence/train_balanced.tsv\")\n",
    "DEV_FILE = Path(\"raw_data/GunViolence/dev.tsv\")\n",
    "TEST_FILE = Path(\"raw_data/GunViolence/test.tsv\")\n",
    "\n",
    "x_train, y_train = load_data_file(TRAIN_FILE)\n",
    "x_dev, y_dev = load_data_file(DEV_FILE)\n",
    "x_test, _ = load_data_file(TEST_FILE, is_test_file=True)\n",
    "\n",
    "# train_posts = df['post']\n",
    "# train_tags = df['tags']\n",
    "\n",
    "# test_posts = df['post'][train_size:]\n",
    "# test_tags = df['tags'][train_size:]\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(x_train) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(x_train)\n",
    "x_test = tokenize.texts_to_matrix(x_dev)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_dev)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(\"num_classes\", num_classes)\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# sgd = optimizers.SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=sgd,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 130us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.112426992563101, 0.6307692527770996]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
